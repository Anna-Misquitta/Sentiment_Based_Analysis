{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhjn8MUA9m15Q1Z7CCa6JZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anna-Misquitta/Sentiment_Based_Analysis/blob/main/model.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDo4XYkAE5Gk",
        "outputId": "4a254731-7346-4fed-ebce-e2b897bccd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import collections\n",
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "import nltk\n",
        "import time\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "class SentimentRecommenderModel:\n",
        "\n",
        "    ROOT_PATH = \"pickle/\"\n",
        "    MODEL_NAME = \"sentiment_classification_xg_boost_model.pkl\"\n",
        "    VECTORIZER = \"tfidf_vectorizer.pkl\"\n",
        "    RECOMMENDER = \"user_final_rating.pkl\"\n",
        "    CLEANED_DATA = \"cleaned_data.pkl\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = pickle.load(open(\n",
        "            SentimentRecommenderModel.ROOT_PATH + SentimentRecommenderModel.MODEL_NAME, 'rb'))\n",
        "        self.vectorizer = pd.read_pickle(\n",
        "            SentimentRecommenderModel.ROOT_PATH + SentimentRecommenderModel.VECTORIZER)\n",
        "        self.user_final_rating = pickle.load(open(\n",
        "            SentimentRecommenderModel.ROOT_PATH + SentimentRecommenderModel.RECOMMENDER, 'rb'))\n",
        "        self.data = pd.read_csv(\"dataset/sample30.csv\")\n",
        "        self.cleaned_data = pickle.load(open(\n",
        "            SentimentRecommenderModel.ROOT_PATH + SentimentRecommenderModel.CLEANED_DATA, 'rb'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    \"\"\"function to get the top product 20 recommendations for the user\"\"\"\n",
        "\n",
        "    def getRecommendationByUser(self, user):\n",
        "        recommedations = []\n",
        "        return list(self.user_final_rating.loc[user].sort_values(ascending=False)[0:20].index)\n",
        "\n",
        "    \"\"\"function to filter the product recommendations using the sentiment model and get the top 5 recommendations\"\"\"\n",
        "\n",
        "    def getSentimentRecommendations(self, user):\n",
        "        if (user in self.user_final_rating.index):\n",
        "            # get the product recommedation using the trained ML model\n",
        "            recommendations = list(\n",
        "                self.user_final_rating.loc[user].sort_values(ascending=False)[0:20].index)\n",
        "            filtered_data = self.cleaned_data[self.cleaned_data.id.isin(\n",
        "                recommendations)]\n",
        "            # preprocess the text before tranforming and predicting\n",
        "            #filtered_data[\"reviews_text_cleaned\"] = filtered_data[\"reviews_text\"].apply(lambda x: self.preprocess_text(x))\n",
        "            # transfor the input data using saved tf-idf vectorizer\n",
        "            X = self.vectorizer.transform(\n",
        "                filtered_data[\"reviews_text_cleaned\"].values.astype(str))\n",
        "            filtered_data[\"predicted_sentiment\"] = self.model.predict(X)\n",
        "            temp = filtered_data[['id', 'predicted_sentiment']]\n",
        "            temp_grouped = temp.groupby('id', as_index=False).count()\n",
        "            temp_grouped[\"pos_review_count\"] = temp_grouped.id.apply(lambda x: temp[(\n",
        "                temp.id == x) & (temp.predicted_sentiment == 1)][\"predicted_sentiment\"].count())\n",
        "            temp_grouped[\"total_review_count\"] = temp_grouped['predicted_sentiment']\n",
        "            temp_grouped['pos_sentiment_percent'] = np.round(\n",
        "                temp_grouped[\"pos_review_count\"]/temp_grouped[\"total_review_count\"]*100, 2)\n",
        "            sorted_products = temp_grouped.sort_values(\n",
        "                'pos_sentiment_percent', ascending=False)[0:5]\n",
        "            return pd.merge(self.data, sorted_products, on=\"id\")[[\"name\", \"brand\", \"manufacturer\", \"pos_sentiment_percent\"]].drop_duplicates().sort_values(['pos_sentiment_percent', 'name'], ascending=[False, True])\n",
        "\n",
        "        else:\n",
        "            print(f\"User name {user} doesn't exist\")\n",
        "            return None\n",
        "\n",
        "    \"\"\"function to classify the sentiment to 1/0 - positive or negative - using the trained ML model\"\"\"\n",
        "\n",
        "    def classify_sentiment(self, review_text):\n",
        "        review_text = self.preprocess_text(review_text)\n",
        "        X = self.vectorizer.transform([review_text])\n",
        "        y_pred = self.model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "    \"\"\"function to preprocess the text before it's sent to ML model\"\"\"\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "\n",
        "        # cleaning the review text (lower, removing punctuation, numericals, whitespaces)\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(\"\\[\\s*\\w*\\s*\\]\", \"\", text)\n",
        "        dictionary = \"abc\".maketrans('', '', string.punctuation)\n",
        "        text = text.translate(dictionary)\n",
        "        text = re.sub(\"\\S*\\d\\S*\", \"\", text)\n",
        "\n",
        "        # remove stop-words and convert it to lemma\n",
        "        text = self.lemma_text(text)\n",
        "        return text\n",
        "\n",
        "    \"\"\"function to get the pos tag to derive the lemma form\"\"\"\n",
        "\n",
        "    def get_wordnet_pos(self, tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    \"\"\"function to remove the stop words from the text\"\"\"\n",
        "\n",
        "    def remove_stopword(self, text):\n",
        "        words = [word for word in text.split() if word.isalpha()\n",
        "                 and word not in self.stop_words]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    \"\"\"function to derive the base lemma form of the text using the pos tag\"\"\"\n",
        "\n",
        "    def lemma_text(self, text):\n",
        "        word_pos_tags = nltk.pos_tag(word_tokenize(\n",
        "            self.remove_stopword(text)))  # Get position tags\n",
        "        # Map the position tag and lemmatize the word/token\n",
        "        words = [self.lemmatizer.lemmatize(tag[0], self.get_wordnet_pos(\n",
        "            tag[1])) for idx, tag in enumerate(word_pos_tags)]\n",
        "        return \" \".join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wMTLXpCE-nj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}